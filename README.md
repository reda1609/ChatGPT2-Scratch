# ChatGPT2-Scratch

## Overview
Here's an implementation of a GPT-2 inspired language model built from scratch using PyTorch. The project follows a step-by-step approach to constructing a Transformer-based language model, including data preprocessing, tokenization, and training on the Tiny Shakespeare dataset.

## Features
- Implements a Bigram Language Model (Baseline Model)
- Uses PyTorch for efficient tensor operations
- Self-attention mechanism for sequence modeling
- Data tokenization and embedding layers
- Implements a Decoder Only Model (GPT-2 Inspired Model)
- Training on a character-level dataset
- Supports text generation

## Architecture
![Model Architecture](https://www.google.com.eg/url?sa=i&url=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2024%2F04%2Fmastering-decoder-only-transformer-a-comprehensive-guide%2F&psig=AOvVaw13xpnnmTAvFhT9oyHaTUN4&ust=1739903127286000&source=images&cd=vfe&opi=89978449&ved=0CBQQjRxqFwoTCPjX5Liqy4sDFQAAAAAdAAAAABAp)

## Future Work
- Implementing a full GPT-2 architecture
- Adding support for byte-pair encoding (BPE)
- Experimenting with larger datasets and improved training techniques

## Contributing
Feel free to fork this repository, submit pull requests, or open issues for any enhancements.